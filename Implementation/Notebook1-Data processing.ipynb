{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82961472-2806-4072-854c-ffa45579e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notebook 1: Data Preprocessing Pipeline\n",
    "# ## Objective: \n",
    "# - Load and merge the 3 Amazon review CSV files\n",
    "# - Clean and preprocess review text (remove noise, normalize, tokenize, etc.)\n",
    "# - Handle missing values and duplicates\n",
    "# - Save preprocessed data for downstream tasks (NER, sentiment analysis, etc.)\n",
    "\n",
    "\n",
    "# ### 1. Import Libraries\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import textblob  # For spelling correction\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load spaCy's English model (for tokenization, lemmatization)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e29821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"datafiniti/consumer-reviews-of-amazon-products\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69485a78-bb89-423b-9f2e-3c3f3289b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 2. Load and Merge Datasets\n",
    "# Your dataset includes 3 CSV files. We’ll load them, check for consistent columns, and merge.\n",
    "# %%\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to dataset\n",
    "path = kagglehub.dataset_download(\"datafiniti/consumer-reviews-of-amazon-products\")\n",
    "\n",
    "# List CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith(\".csv\")]\n",
    "csv_files.sort()  # optional: to ensure consistent order\n",
    "\n",
    "# Load each CSV into a DataFrame\n",
    "df1 = pd.read_csv(os.path.join(path, csv_files[0]))\n",
    "df2 = pd.read_csv(os.path.join(path, csv_files[1]))\n",
    "df3 = pd.read_csv(os.path.join(path, csv_files[2]))\n",
    "\n",
    "print(\"Loaded files:\")\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959c8416-a206-4fee-bd88-4ffdfde00763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df1: ['id', 'name', 'asins', 'brand', 'categories', 'keys', 'manufacturer', 'reviews.date', 'reviews.dateAdded', 'reviews.dateSeen', 'reviews.didPurchase', 'reviews.doRecommend', 'reviews.id', 'reviews.numHelpful', 'reviews.rating', 'reviews.sourceURLs', 'reviews.text', 'reviews.title', 'reviews.userCity', 'reviews.userProvince', 'reviews.username']\n",
      "\n",
      "Columns in df2: ['id', 'dateAdded', 'dateUpdated', 'name', 'asins', 'brand', 'categories', 'primaryCategories', 'imageURLs', 'keys', 'manufacturer', 'manufacturerNumber', 'reviews.date', 'reviews.dateAdded', 'reviews.dateSeen', 'reviews.doRecommend', 'reviews.id', 'reviews.numHelpful', 'reviews.rating', 'reviews.sourceURLs', 'reviews.text', 'reviews.title', 'reviews.username', 'sourceURLs']\n",
      "\n",
      "Columns in df3: ['id', 'dateAdded', 'dateUpdated', 'name', 'asins', 'brand', 'categories', 'primaryCategories', 'imageURLs', 'keys', 'manufacturer', 'manufacturerNumber', 'reviews.date', 'reviews.dateSeen', 'reviews.didPurchase', 'reviews.doRecommend', 'reviews.id', 'reviews.numHelpful', 'reviews.rating', 'reviews.sourceURLs', 'reviews.text', 'reviews.title', 'reviews.username', 'sourceURLs']\n"
     ]
    }
   ],
   "source": [
    "# #### 2.1 Inspect Columns (Ensure Consistency)\n",
    "# Amazon review datasets often have columns like `reviewText`, `rating`, `product_name`, etc. We need to standardize column names.\n",
    "# %%\n",
    "print(\"Columns in df1:\", df1.columns.tolist())\n",
    "print(\"\\nColumns in df2:\", df2.columns.tolist())\n",
    "print(\"\\nColumns in df3:\", df3.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51739425-cde3-4709-8b5f-b8377ae611d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized columns (all DataFrames): ['product_name', 'review_text', 'rating', 'review_date']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #### 2.1 Standardize Columns (Critical for Merging)\n",
    "# Based on your column lists, all 3 DataFrames share these key columns:\n",
    "# - `name`: Product name  \n",
    "# - `reviews.text`: Raw review content  \n",
    "# - `reviews.rating`: Star rating (1-5)  \n",
    "# - `reviews.date`: Date of the review  \n",
    "\n",
    "# We’ll rename these to simpler labels and keep only these columns for our tasks.\n",
    "# %%\n",
    "def standardize_columns(df):\n",
    "    \"\"\"Standardize column names and keep only relevant columns.\"\"\"\n",
    "    # Rename columns to intuitive labels\n",
    "    column_mapping = {\n",
    "        \"name\": \"product_name\",          # Product name\n",
    "        \"reviews.text\": \"review_text\",   # Raw review content\n",
    "        \"reviews.rating\": \"rating\",      # Star rating (1-5)\n",
    "        \"reviews.date\": \"review_date\"    # Date of the review\n",
    "    }\n",
    "    # Rename columns and keep only the mapped ones\n",
    "    df_standardized = df.rename(columns=column_mapping)[column_mapping.values()]\n",
    "    return df_standardized\n",
    "\n",
    "# Apply standardization to all DataFrames\n",
    "df1_clean = standardize_columns(df1)\n",
    "df2_clean = standardize_columns(df2)\n",
    "df3_clean = standardize_columns(df3)\n",
    "\n",
    "# Verify column consistency after standardization\n",
    "print(\"Standardized columns (all DataFrames):\", df1_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "672ee2e6-070b-487d-86cd-9407d3866e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews after merging: 67,992\n"
     ]
    }
   ],
   "source": [
    "# #### 2.2 Merge Datasets and Check Size\n",
    "# %%\n",
    "# Merge all 3 DataFrames\n",
    "combined_df = pd.concat([df1_clean, df2_clean, df3_clean], ignore_index=True)\n",
    "print(f\"Total reviews after merging: {len(combined_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f305cfa-a125-4c1c-8634-fe52e2ef1d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before cleaning:\n",
      "product_name    6760\n",
      "review_text        1\n",
      "rating            33\n",
      "review_date       39\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ### 3. Handle Missing Values and Duplicates\n",
    "# Critical for reliable NLP: Remove rows with missing review text or ratings, and drop duplicate reviews.\n",
    "# %%\n",
    "# Check missing values in key columns\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(combined_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd956738-ae72-4182-8e4c-e1845015665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total reviews after cleaning: 64,037\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing review text or ratings (core for NLP tasks)\n",
    "combined_df = combined_df.dropna(subset=[\"review_text\", \"rating\"]).reset_index(drop=True)\n",
    "\n",
    "# Drop duplicate reviews (same product + same review text = redundant)\n",
    "combined_df = combined_df.drop_duplicates(subset=[\"product_name\", \"review_text\"], keep=\"first\")\n",
    "\n",
    "# Convert rating to numeric (in case it’s stored as string)\n",
    "combined_df[\"rating\"] = pd.to_numeric(combined_df[\"rating\"], errors=\"coerce\")\n",
    "# Drop any remaining non-numeric ratings\n",
    "combined_df = combined_df.dropna(subset=[\"rating\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTotal reviews after cleaning: {len(combined_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f10b5d28-a7ed-47eb-acdf-18a2b9b3ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 4. Text Preprocessing Pipeline\n",
    "# Clean and normalize `review_text` for downstream tasks (NER, sentiment analysis, etc.):\n",
    "# 1. Remove URLs, special characters, and numbers  \n",
    "# 2. Correct minor spelling errors  \n",
    "# 3. Tokenize, remove stopwords (e.g., \"the\", \"and\"), and lemmatize (e.g., \"running\" → \"run\")  \n",
    "# %%\n",
    "def clean_text(raw_text):\n",
    "    \"\"\"Step 1: Remove noise (URLs, special characters, numbers).\"\"\"\n",
    "    # Remove URLs (e.g., \"https://example.com\")\n",
    "    text = re.sub(r\"http\\S+\", \"\", raw_text)\n",
    "    # Remove numbers and special characters (keep only letters and spaces)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", raw_text)\n",
    "    # Collapse multiple spaces into one\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"Step 2: Correct minor spelling errors (e.g., \"teh\" → \"the\").\"\"\"\n",
    "    # Limit correction to short texts to avoid slow processing\n",
    "    if len(text) > 500:  # Skip long texts (speed optimization)\n",
    "        return text\n",
    "    return str(textblob.TextBlob(text).correct())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Full preprocessing: clean → spell check → tokenize → lemmatize → remove stopwords.\"\"\"\n",
    "    # Step 1: Clean text\n",
    "    cleaned = clean_text(text)\n",
    "    if not cleaned:  # Skip empty texts after cleaning\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 2: Correct spelling (optional but improves NER accuracy)\n",
    "    spelled_correct = correct_spelling(cleaned)\n",
    "    \n",
    "    # Step 3: Process with spaCy (lowercasing, tokenization, lemmatization)\n",
    "    doc = nlp(spelled_correct.lower())  # Convert to lowercase first\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Skip stopwords (e.g., \"is\", \"and\") and punctuation\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        # Keep lemmatized form (base word)\n",
    "        tokens.append(token.lemma_)\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6e78db2-0806-4652-a7df-a3d37a95e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing reviews:  37%|███▋      | 23856/64037 [1:07:36<7:07:01,  1.57it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Preprocessing reviews: 100%|██████████| 64037/64037 [2:49:45<00:00,  6.29it/s]   \n"
     ]
    }
   ],
   "source": [
    "# #### 4.1 Apply Preprocessing to All Reviews\n",
    "# *Note: This may take 10-15 minutes for 30k+ reviews. Use `tqdm` to track progress.*\n",
    "# %%\n",
    "# Apply preprocessing with a progress bar\n",
    "tqdm.pandas(desc=\"Preprocessing reviews\")\n",
    "combined_df[\"cleaned_review\"] = combined_df[\"review_text\"].progress_apply(preprocess_text)\n",
    "\n",
    "# Remove rows where preprocessing resulted in empty text (rare, but possible)\n",
    "combined_df = combined_df[combined_df[\"cleaned_review\"] != \"\"].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1995f0b8-25cb-458c-ac44-02d314c101e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment label distribution:\n",
      "sentiment_label\n",
      "1    91.9\n",
      "2     4.3\n",
      "0     3.8\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ### 5. Create Sentiment Labels (for Sentiment Analysis)\n",
    "# Map numeric ratings to sentiment categories:\n",
    "# - Rating ≥ 4 → Positive (1)  \n",
    "# - Rating ≤ 2 → Negative (0)  \n",
    "# - Rating = 3 → Neutral (2)  \n",
    "# %%\n",
    "def get_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return 1  # Positive\n",
    "    elif rating <= 2:\n",
    "        return 0  # Negative\n",
    "    else:\n",
    "        return 2  # Neutral\n",
    "\n",
    "combined_df[\"sentiment_label\"] = combined_df[\"rating\"].apply(get_sentiment)\n",
    "\n",
    "# Check distribution of sentiment labels\n",
    "print(\"\\nSentiment label distribution:\")\n",
    "print(combined_df[\"sentiment_label\"].value_counts(normalize=True).round(3) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfdbeaa8-8c27-4526-be1e-6f8eb84b0028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed data saved as 'preprocessed_amazon_reviews.csv'\n"
     ]
    }
   ],
   "source": [
    "# ### 6. Save Preprocessed Data\n",
    "# Save the cleaned dataset for use in Notebook 2 (Basic NLP Techniques) and beyond.\n",
    "# %%\n",
    "combined_df.to_csv(\"preprocessed_amazon_reviews.csv\", index=False)\n",
    "print(\"\\nPreprocessed data saved as 'preprocessed_amazon_reviews.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e59cabe-82bf-45ce-ad8f-35bd4fc37107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35292</th>\n",
       "      <td>Amazon Echo Show Alexa-enabled Bluetooth Speak...</td>\n",
       "      <td>I am so happy to have the Echo Show. I have be...</td>\n",
       "      <td>happy echo able listen christmas song country ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55943</th>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>Holds a long charge. Fast great space, no free...</td>\n",
       "      <td>fold long charge great space freezing resecting</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53856</th>\n",
       "      <td>Fire Kids Edition Tablet, 7 Display, Wi-Fi, 16...</td>\n",
       "      <td>Kindle Fire meets the needs of kids and/or adu...</td>\n",
       "      <td>kindle fire meet need kiss ardor adult</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes...</td>\n",
       "      <td>This tablet is a great purchase for the price....</td>\n",
       "      <td>tablet great purchase price easy use perfect kiss</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23816</th>\n",
       "      <td>Echo (White),,,\\r\\nEcho (White),,,</td>\n",
       "      <td>Fun to play with.. Was alittle difficult to ge...</td>\n",
       "      <td>run play little difficult set</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            product_name  \\\n",
       "35292  Amazon Echo Show Alexa-enabled Bluetooth Speak...   \n",
       "55943  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "53856  Fire Kids Edition Tablet, 7 Display, Wi-Fi, 16...   \n",
       "7971   Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes...   \n",
       "23816                 Echo (White),,,\\r\\nEcho (White),,,   \n",
       "\n",
       "                                             review_text  \\\n",
       "35292  I am so happy to have the Echo Show. I have be...   \n",
       "55943  Holds a long charge. Fast great space, no free...   \n",
       "53856  Kindle Fire meets the needs of kids and/or adu...   \n",
       "7971   This tablet is a great purchase for the price....   \n",
       "23816  Fun to play with.. Was alittle difficult to ge...   \n",
       "\n",
       "                                          cleaned_review  rating  \\\n",
       "35292  happy echo able listen christmas song country ...     5.0   \n",
       "55943    fold long charge great space freezing resecting     5.0   \n",
       "53856             kindle fire meet need kiss ardor adult     5.0   \n",
       "7971   tablet great purchase price easy use perfect kiss     4.0   \n",
       "23816                      run play little difficult set     4.0   \n",
       "\n",
       "       sentiment_label  \n",
       "35292                1  \n",
       "55943                1  \n",
       "53856                1  \n",
       "7971                 1  \n",
       "23816                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ### 7. Validate Preprocessing Quality\n",
    "# Spot-check 5 random reviews to ensure cleaning worked as expected.\n",
    "# %%\n",
    "# Random sample of 5 reviews (raw vs. cleaned)\n",
    "sample = combined_df.sample(5)[[\n",
    "    \"product_name\", \n",
    "    \"review_text\", \n",
    "    \"cleaned_review\", \n",
    "    \"rating\", \n",
    "    \"sentiment_label\"\n",
    "]]\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e1f81-839f-4437-a311-0df06749be91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ba40e-3df0-4128-b4e2-742889778b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
